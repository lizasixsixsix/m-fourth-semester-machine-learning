{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mo-2-4-0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJbO6Hqtaukq",
        "colab_type": "text"
      },
      "source": [
        "# Лабораторная работа №4\n",
        "\n",
        "## Реализация приложения по распознаванию номеров домов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jBSK2FabPKC",
        "colab_type": "text"
      },
      "source": [
        "Набор изображений из _Google Street View_ с изображениями номеров домов, содержащий 10 классов, соответствующих цифрам от 0 до 9.\n",
        "\n",
        "* 73257 изображений цифр в обучающей выборке;\n",
        "\n",
        "* 26032 изображения цифр в тестовой выборке;\n",
        "\n",
        "* 531131 изображения, которые можно использовать как дополнение к обучающей выборке;\n",
        "\n",
        "* В двух форматах:\n",
        "\n",
        "    * Оригинальные изображения с выделенными цифрами;\n",
        "\n",
        "    * Изображения размером 32×32, содержащие одну цифру;\n",
        "\n",
        "* Данные первого формата можно скачать по ссылкам:\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/train.tar.gz (обучающая выборка);\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/test.tar.gz (тестовая выборка);\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/extra.tar.gz (дополнительные данные);\n",
        "\n",
        "* Данные второго формата можно скачать по ссылкам:\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/train_32x32.mat (обучающая выборка);\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/test_32x32.mat (тестовая выборка);\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/extra_32x32.mat (дополнительные данные);\n",
        "\n",
        "* Описание данных на английском языке доступно по ссылке:\n",
        "\n",
        "    * http://ufldl.stanford.edu/housenumbers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8r0rcrSbum0",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Реализуйте глубокую нейронную сеть (полносвязную или сверточную) и обучите ее на синтетических данных (например, наборы _MNIST_ (http://yann.lecun.com/exdb/mnist/) или _notMNIST_).\n",
        "\n",
        "Ознакомьтесь с имеющимися работами по данной тематике: англоязычная статья ( http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf ), видео на _YouTube_ (https://www.youtube.com/watch?v=vGPI_JvLoN0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwZ5YedLSmq5",
        "colab_type": "text"
      },
      "source": [
        "Используем архитектуру _LeNet-5_ и обучим сеть сначала на данных из набора _MNIST_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5VyFAn6oppw",
        "colab_type": "code",
        "outputId": "856c543d-fcff-454a-f736-de1f1ca53151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "! pip install tensorflow-gpu --pre --quiet\n",
        "\n",
        "! pip show tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow-gpu\n",
            "Version: 2.2.0rc3\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: grpcio, tensorboard, termcolor, six, gast, absl-py, tensorflow-estimator, opt-einsum, wrapt, wheel, numpy, google-pasta, protobuf, scipy, astunparse, keras-preprocessing, h5py\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A6ova40mePh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxMOj2NcyUrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqe79JL0mGNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEaLeEAKy-sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test = tf.keras.utils.normalize(x_train, axis = 1), tf.keras.utils.normalize(x_test, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDy2wUQwyQFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA-LEJ-DH85n",
        "colab_type": "code",
        "outputId": "cec7b746-a3bf-4e99-80a1-7cbe5d07f12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "y_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4-m2kcv1bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_DIM_0, IMAGE_DIM_1 = x_train.shape[1], x_train.shape[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8FxR1mymxWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES_N = y_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfSn_LKOm6U_",
        "colab_type": "code",
        "outputId": "a7b1a881-483e-48c7-d361-33bd1352e4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJQ6uXanJQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import AveragePooling2D, Conv2D, Dense, Flatten\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Conv2D(6, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'same',\n",
        "                   input_shape = (IMAGE_DIM_0, IMAGE_DIM_1, 1)))\n",
        "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(Conv2D(16, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'valid'))\n",
        "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation = 'tanh'))\n",
        "model.add(Dense(84, activation = 'tanh'))\n",
        "model.add(Dense(CLASSES_N, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF5wW8VYnca7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'sparse_categorical_crossentropy' gave NAN loss\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiqd4HCfnezG",
        "colab_type": "code",
        "outputId": "82ba4e91-dd93-4f5b-9922-ae04fc1b16e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               48120     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnv6sb5Tniww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS_N = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWfwaspGnsVK",
        "colab_type": "code",
        "outputId": "5bc5a260-d7a7-443c-ef1c-9567405d8480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "model.fit(x = x_train, y = y_train, validation_split = 0.15, epochs = EPOCHS_N)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.2895 - categorical_accuracy: 0.9129 - val_loss: 0.1330 - val_categorical_accuracy: 0.9591\n",
            "Epoch 2/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.1186 - categorical_accuracy: 0.9639 - val_loss: 0.0995 - val_categorical_accuracy: 0.9694\n",
            "Epoch 3/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0801 - categorical_accuracy: 0.9748 - val_loss: 0.0960 - val_categorical_accuracy: 0.9697\n",
            "Epoch 4/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0631 - categorical_accuracy: 0.9799 - val_loss: 0.0737 - val_categorical_accuracy: 0.9786\n",
            "Epoch 5/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0484 - categorical_accuracy: 0.9848 - val_loss: 0.0679 - val_categorical_accuracy: 0.9802\n",
            "Epoch 6/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0394 - categorical_accuracy: 0.9874 - val_loss: 0.0647 - val_categorical_accuracy: 0.9806\n",
            "Epoch 7/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0328 - categorical_accuracy: 0.9894 - val_loss: 0.0688 - val_categorical_accuracy: 0.9811\n",
            "Epoch 8/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0268 - categorical_accuracy: 0.9919 - val_loss: 0.0698 - val_categorical_accuracy: 0.9806\n",
            "Epoch 9/20\n",
            "1594/1594 [==============================] - 4s 3ms/step - loss: 0.0242 - categorical_accuracy: 0.9921 - val_loss: 0.0617 - val_categorical_accuracy: 0.9822\n",
            "Epoch 10/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0197 - categorical_accuracy: 0.9935 - val_loss: 0.0687 - val_categorical_accuracy: 0.9822\n",
            "Epoch 11/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0168 - categorical_accuracy: 0.9944 - val_loss: 0.0719 - val_categorical_accuracy: 0.9811\n",
            "Epoch 12/20\n",
            "1594/1594 [==============================] - 4s 3ms/step - loss: 0.0155 - categorical_accuracy: 0.9948 - val_loss: 0.0680 - val_categorical_accuracy: 0.9808\n",
            "Epoch 13/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0137 - categorical_accuracy: 0.9955 - val_loss: 0.0679 - val_categorical_accuracy: 0.9822\n",
            "Epoch 14/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0121 - categorical_accuracy: 0.9965 - val_loss: 0.0768 - val_categorical_accuracy: 0.9806\n",
            "Epoch 15/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0115 - categorical_accuracy: 0.9961 - val_loss: 0.0747 - val_categorical_accuracy: 0.9824\n",
            "Epoch 16/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0122 - categorical_accuracy: 0.9958 - val_loss: 0.0687 - val_categorical_accuracy: 0.9840\n",
            "Epoch 17/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0095 - categorical_accuracy: 0.9966 - val_loss: 0.0633 - val_categorical_accuracy: 0.9848\n",
            "Epoch 18/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0100 - categorical_accuracy: 0.9965 - val_loss: 0.0701 - val_categorical_accuracy: 0.9836\n",
            "Epoch 19/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0055 - categorical_accuracy: 0.9984 - val_loss: 0.0706 - val_categorical_accuracy: 0.9842\n",
            "Epoch 20/20\n",
            "1594/1594 [==============================] - 5s 3ms/step - loss: 0.0093 - categorical_accuracy: 0.9967 - val_loss: 0.0714 - val_categorical_accuracy: 0.9838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60025300f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jEk5lajSKKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3aae7c08-0dd3-4fa3-d175-2ac34c74fde3"
      },
      "source": [
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test loss, test accuracy:', results)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0828 - categorical_accuracy: 0.9808\n",
            "Test loss, test accuracy: [0.08280119299888611, 0.9807999730110168]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_YPL2eNS-IW",
        "colab_type": "text"
      },
      "source": [
        "Удалось достичь отличного результата &mdash; точность распознавания на тестовой выборке составила 98,0%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhEJUbH-b9RT",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "После уточнения модели на синтетических данных попробуйте обучить ее на реальных данных (набор _Google Street View_). Что изменилось в модели?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpjg0mUPPZzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DS_URL_FOLDER = 'http://ufldl.stanford.edu/housenumbers/'\n",
        "\n",
        "FIRST_DS_EXT = '.tar.gz'\n",
        "SECOND_DS_EXT = '_32x32.mat'\n",
        "\n",
        "TRAIN_DS_NAME = 'train'\n",
        "TEST_DS_NAME = 'test'\n",
        "EXTRA_DS_NAME = 'extra'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1Au9pWQ1gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "def load_file(_url_folder, _name, _ext, _key, _local_ext = ''):\n",
        "\n",
        "    file_url_ = _url_folder + _name + _ext\n",
        "\n",
        "    local_file_name_ = _name + '_' + _key + _local_ext\n",
        "\n",
        "    urlretrieve(file_url_, local_file_name_)\n",
        "\n",
        "    return local_file_name_\n",
        "\n",
        "def tar_gz_to_dir(_url_folder, _name, _ext, _key):\n",
        "\n",
        "    local_file_name_ = load_file(_url_folder, _name, _ext, _key, _ext)\n",
        "\n",
        "    dir_name_ = _name + '_' + _key\n",
        "    \n",
        "    with tarfile.open(local_file_name_, 'r:gz') as tar_:\n",
        "        tar_.extractall(dir_name_)\n",
        "\n",
        "    os.remove(local_file_name_)\n",
        "\n",
        "    return dir_name_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMhgLBpdQ2M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_ds_train_dir = tar_gz_to_dir(DS_URL_FOLDER, TRAIN_DS_NAME, FIRST_DS_EXT, 'first')\n",
        "first_ds_test_dir = tar_gz_to_dir(DS_URL_FOLDER, TEST_DS_NAME, FIRST_DS_EXT, 'first')\n",
        "first_ds_extra_dir = tar_gz_to_dir(DS_URL_FOLDER, EXTRA_DS_NAME, FIRST_DS_EXT, 'first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY9H9gOx_agn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_ds_train_file = load_file(DS_URL_FOLDER, TRAIN_DS_NAME, SECOND_DS_EXT, 'second')\n",
        "second_ds_test_file = load_file(DS_URL_FOLDER, TEST_DS_NAME, SECOND_DS_EXT, 'second')\n",
        "second_ds_extra_file = load_file(DS_URL_FOLDER, EXTRA_DS_NAME, SECOND_DS_EXT, 'second')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmvtvvHjWam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import io\n",
        "\n",
        "second_ds_train = io.loadmat(second_ds_train_file)\n",
        "second_ds_test = io.loadmat(second_ds_test_file)\n",
        "second_ds_extra = io.loadmat(second_ds_extra_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aZehuSyjtMa",
        "colab_type": "code",
        "outputId": "31eb2dac-80c3-4268-da8f-a01d6efc15ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "X_second_ds_train = np.moveaxis(second_ds_train['X'], -1, 0)\n",
        "X_second_ds_test = np.moveaxis(second_ds_test['X'], -1, 0)\n",
        "X_second_ds_extra = np.moveaxis(second_ds_extra['X'], -1, 0)\n",
        "\n",
        "y_second_ds_train = second_ds_train['y']\n",
        "y_second_ds_test = second_ds_test['y']\n",
        "y_second_ds_extra = second_ds_extra['y']\n",
        "\n",
        "print(X_second_ds_train.shape, y_second_ds_train.shape)\n",
        "print(X_second_ds_test.shape, y_second_ds_test.shape)\n",
        "print(X_second_ds_extra.shape, y_second_ds_extra.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(73257, 32, 32, 3) (73257, 1)\n",
            "(26032, 32, 32, 3) (26032, 1)\n",
            "(531131, 32, 32, 3) (531131, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rGezUtOltjt",
        "colab_type": "code",
        "outputId": "5675a4d2-ee5b-47e4-fdac-173595f74d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(X_second_ds_train[100])\n",
        "plt.imshow(X_second_ds_test[100])\n",
        "plt.imshow(X_second_ds_extra[100])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6002399320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUuo9YEP3w0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_DIM_0_2, IMAGE_DIM_1_2, IMAGE_DIM_2_2 = X_second_ds_train.shape[-3], X_second_ds_train.shape[-2], X_second_ds_train.shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnfDnP9D5U_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_second_ds_train_cat = to_categorical(y_second_ds_train)\n",
        "y_second_ds_test_cat = to_categorical(y_second_ds_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuoxju15AGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES_N_2 = y_second_ds_train_cat.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH96BC9-3s7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = tf.keras.Sequential()\n",
        "\n",
        "model_2.add(Conv2D(6, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'same',\n",
        "                   input_shape = (IMAGE_DIM_0_2, IMAGE_DIM_1_2, IMAGE_DIM_2_2)))\n",
        "model_2.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model_2.add(Conv2D(16, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'valid'))\n",
        "model_2.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(120, activation = 'tanh'))\n",
        "model_2.add(Dense(84, activation = 'tanh'))\n",
        "model_2.add(Dense(CLASSES_N_2, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxY6LEWR3q33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.compile(optimizer = 'adam',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubm_QyIZ3qCV",
        "colab_type": "code",
        "outputId": "ce09622a-10c7-4bf1-ddc7-cdf1ffc45cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 6)         456       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 16, 16, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 16)        2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 120)               69240     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 11)                935       \n",
            "=================================================================\n",
            "Total params: 83,211\n",
            "Trainable params: 83,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAONS2Z92v8g",
        "colab_type": "code",
        "outputId": "1ddc7344-1a4f-4dd5-e717-43e8eeaf5a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "model_2.fit(x = X_second_ds_train, y = y_second_ds_train_cat, validation_split = 0.15, epochs = EPOCHS_N)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 1.1878 - categorical_accuracy: 0.6141 - val_loss: 0.8286 - val_categorical_accuracy: 0.7351\n",
            "Epoch 2/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.6917 - categorical_accuracy: 0.7843 - val_loss: 0.6128 - val_categorical_accuracy: 0.8132\n",
            "Epoch 3/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.5979 - categorical_accuracy: 0.8130 - val_loss: 0.5611 - val_categorical_accuracy: 0.8280\n",
            "Epoch 4/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.5475 - categorical_accuracy: 0.8287 - val_loss: 0.5569 - val_categorical_accuracy: 0.8276\n",
            "Epoch 5/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.5193 - categorical_accuracy: 0.8383 - val_loss: 0.5476 - val_categorical_accuracy: 0.8333\n",
            "Epoch 6/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.4886 - categorical_accuracy: 0.8474 - val_loss: 0.5300 - val_categorical_accuracy: 0.8375\n",
            "Epoch 7/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.4755 - categorical_accuracy: 0.8511 - val_loss: 0.5751 - val_categorical_accuracy: 0.8284\n",
            "Epoch 8/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.4482 - categorical_accuracy: 0.8613 - val_loss: 0.5043 - val_categorical_accuracy: 0.8469\n",
            "Epoch 9/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.4372 - categorical_accuracy: 0.8642 - val_loss: 0.5296 - val_categorical_accuracy: 0.8393\n",
            "Epoch 10/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.4162 - categorical_accuracy: 0.8697 - val_loss: 0.5304 - val_categorical_accuracy: 0.8381\n",
            "Epoch 11/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.4058 - categorical_accuracy: 0.8733 - val_loss: 0.5445 - val_categorical_accuracy: 0.8385\n",
            "Epoch 12/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.4027 - categorical_accuracy: 0.8753 - val_loss: 0.5371 - val_categorical_accuracy: 0.8347\n",
            "Epoch 13/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3966 - categorical_accuracy: 0.8747 - val_loss: 0.5472 - val_categorical_accuracy: 0.8381\n",
            "Epoch 14/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3922 - categorical_accuracy: 0.8759 - val_loss: 0.5105 - val_categorical_accuracy: 0.8482\n",
            "Epoch 15/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3835 - categorical_accuracy: 0.8784 - val_loss: 0.5198 - val_categorical_accuracy: 0.8477\n",
            "Epoch 16/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3669 - categorical_accuracy: 0.8850 - val_loss: 0.4903 - val_categorical_accuracy: 0.8523\n",
            "Epoch 17/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3616 - categorical_accuracy: 0.8858 - val_loss: 0.5543 - val_categorical_accuracy: 0.8359\n",
            "Epoch 18/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.3559 - categorical_accuracy: 0.8873 - val_loss: 0.5141 - val_categorical_accuracy: 0.8467\n",
            "Epoch 19/20\n",
            "1946/1946 [==============================] - 6s 3ms/step - loss: 0.3472 - categorical_accuracy: 0.8914 - val_loss: 0.5160 - val_categorical_accuracy: 0.8511\n",
            "Epoch 20/20\n",
            "1946/1946 [==============================] - 5s 3ms/step - loss: 0.3439 - categorical_accuracy: 0.8917 - val_loss: 0.6758 - val_categorical_accuracy: 0.7925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60022ca4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYW78QJPSfMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "24e3d481-b263-4d48-a59a-66887da67fe6"
      },
      "source": [
        "results = model_2.evaluate(X_second_ds_test, y_second_ds_test_cat)\n",
        "\n",
        "print('Test loss, test accuracy:', results)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 1s 2ms/step - loss: 0.7207 - categorical_accuracy: 0.7746\n",
            "Test loss, test accuracy: [0.7207155227661133, 0.7745851278305054]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLwTArXRTZY8",
        "colab_type": "text"
      },
      "source": [
        "Прежде всего, в модели изменилось то, что добавился ещё один класс &mdash; _не распознано_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q-E6_NBTVOD",
        "colab_type": "text"
      },
      "source": [
        "Эти данные более сложны для распознавания, что повлияло на результат &mdash; точность распознавания на тестовой выборке составила 77,4%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI_e3nVUcCXb",
        "colab_type": "text"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Сделайте множество снимков изображений номеров домов с помощью смартфона на ОС _Android_. Также можно использовать библиотеки _OpenCV_, _Simple CV_ или _Pygame_ для обработки изображений с общедоступных камер видеонаблюдения (например, https://www.earthcam.com/).\n",
        "\n",
        "В качестве примера использования библиотеки _TensorFlow_ на смартфоне можете воспользоваться демонстрационным приложением от _Google_ (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDRp62kQcQd8",
        "colab_type": "text"
      },
      "source": [
        "### Задание 4\n",
        "\n",
        "Реализуйте приложение для ОС _Android_, которое может распознавать цифры в номерах домов, используя разработанный ранее классификатор. Какова доля правильных классификаций?"
      ]
    }
  ]
}