{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mo-2-6-0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cfSTC_fExd",
        "colab_type": "text"
      },
      "source": [
        "# Лабораторная работа №6\n",
        "\n",
        "## Применение сверточных нейронных сетей (многоклассовая классификация)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj2xTHG6fYhe",
        "colab_type": "text"
      },
      "source": [
        "Набор данных для распознавания языка жестов, который состоит из изображений размерности 28x28 в оттенках серого (значение пикселя от 0 до 255).\n",
        "\n",
        "Каждое из изображений обозначает букву латинского алфавита, обозначенную с помощью жеста (изображения в наборе данных в оттенках серого).\n",
        "\n",
        "Обучающая выборка включает в себя 27,455 изображений, а контрольная выборка содержит 7172 изображения.\n",
        "\n",
        "Данные в виде _csv_-файлов можно скачать на сайте _Kaggle_: https://www.kaggle.com/datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R2bvokTfwuG",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Загрузите данные. Разделите исходный набор данных на обучающую и валидационную выборки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSU2rJJGrn5s",
        "colab_type": "code",
        "outputId": "247da068-8f7c-4956-e49c-63c79a2eea1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw--OkDXry4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/content/drive/My Drive/Colab Files/mo-2'\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLSt5UKjr2q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_ARCHIVE_NAME = 'sign-language-mnist.zip'\n",
        "\n",
        "LOCAL_DIR_NAME = 'sign-language'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRDKuhNDsHcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(os.path.join(BASE_DIR, DATA_ARCHIVE_NAME), 'r') as zip_:\n",
        "    zip_.extractall(path = os.path.join(LOCAL_DIR_NAME, 'train'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eaDS5TrsMIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_FILE_PATH = 'sign-language/train/sign_mnist_train.csv'\n",
        "TEST_FILE_PATH = 'sign-language/train/sign_mnist_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZdjccQssbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_FILE_PATH)\n",
        "test_df = pd.read_csv(TEST_FILE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olZmSdvitTTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_df = pd.concat([train_df, test_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIs-JqDptwi7",
        "colab_type": "code",
        "outputId": "02f3dee2-53a3-4665-bc2b-8893e93ba7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(all_df.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34627, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTptMAe7IFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_DIM = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxYJPtuG7jpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def row_to_label(_row):\n",
        "    return _row[0]\n",
        "\n",
        "def row_to_one_image(_row):\n",
        "    return _row[1:].values.reshape((IMAGE_DIM, IMAGE_DIM, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Fmd0L49EEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_images_and_labels(_dataframe):\n",
        "\n",
        "    llll = _dataframe.apply(lambda row: row_to_label(row), axis = 1)\n",
        "    mmmm = _dataframe.apply(lambda row: row_to_one_image(row), axis = 1)\n",
        "\n",
        "    data_dict_ = { 'label': llll, 'image': mmmm }\n",
        "\n",
        "    reshaped_ = pd.DataFrame(data_dict_, columns = ['label', 'image'])\n",
        "\n",
        "    return reshaped_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PeWh5EJM6pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_df_reshaped = to_images_and_labels(all_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0p2LpcBf02u",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "Реализуйте глубокую нейронную сеть со сверточными слоями. Какое качество классификации получено? Какая архитектура сети была использована?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlORE3O0FQQ8",
        "colab_type": "text"
      },
      "source": [
        "Возьмём _LeNet-5_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxG1n5fUJP9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "b84f6566-5def-45ee-f1ea-290f89ec2115"
      },
      "source": [
        "! pip install tensorflow-gpu --pre --quiet\n",
        "\n",
        "! pip show tensorflow-gpu"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 516.2MB 28kB/s \n",
            "\u001b[?25hName: tensorflow-gpu\n",
            "Version: 2.2.0rc3\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: termcolor, wrapt, numpy, six, google-pasta, scipy, absl-py, wheel, opt-einsum, gast, tensorflow-estimator, h5py, astunparse, protobuf, tensorboard, keras-preprocessing, grpcio\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wDnBN8RACua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37bf1f20-2cd7-49c6-e294-95f9b92a9c9c"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "X = np.asarray(list(all_df_reshaped['image']))\n",
        "\n",
        "y = to_categorical(all_df_reshaped['label'].astype('category').cat.codes.astype('int32'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34627, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8mEQ38cA9Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES_N = y.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Ium3RsJTM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0cFJ4s5FN2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import AveragePooling2D, Conv2D, Dense, Flatten\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Conv2D(6, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'same',\n",
        "                   input_shape = (IMAGE_DIM, IMAGE_DIM, 1)))\n",
        "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(Conv2D(16, kernel_size = (5, 5), strides = (1, 1), activation = 'tanh', padding = 'valid'))\n",
        "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation = 'tanh'))\n",
        "model.add(Dense(84, activation = 'tanh'))\n",
        "model.add(Dense(CLASSES_N, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo7TE0JOGabq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYpxqvQMGiy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0896e72b-7ad9-4fec-f515-ad5e9133965c"
      },
      "source": [
        "model.fit(x = X, y = y, epochs = 50, validation_split = 0.15)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 0.6113 - categorical_accuracy: 0.8472 - val_loss: 0.2022 - val_categorical_accuracy: 0.9577\n",
            "Epoch 2/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0447 - categorical_accuracy: 0.9958 - val_loss: 0.0398 - val_categorical_accuracy: 0.9948\n",
            "Epoch 3/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0101 - categorical_accuracy: 0.9994 - val_loss: 0.0102 - val_categorical_accuracy: 0.9996\n",
            "Epoch 4/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 0.0050 - val_categorical_accuracy: 0.9998\n",
            "Epoch 5/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0043 - val_categorical_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 0.0797 - categorical_accuracy: 0.9776 - val_loss: 0.0199 - val_categorical_accuracy: 0.9977\n",
            "Epoch 7/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 0.0053 - categorical_accuracy: 0.9993 - val_loss: 0.0042 - val_categorical_accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 8.8589e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0023 - val_categorical_accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 5.0702e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 3.3111e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.1518e-04 - categorical_accuracy: 1.0000 - val_loss: 8.1305e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.4198e-04 - categorical_accuracy: 1.0000 - val_loss: 7.5987e-04 - val_categorical_accuracy: 0.9998\n",
            "Epoch 13/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 9.2477e-05 - categorical_accuracy: 1.0000 - val_loss: 5.5920e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 5.8950e-05 - categorical_accuracy: 1.0000 - val_loss: 3.2095e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 3.8223e-05 - categorical_accuracy: 1.0000 - val_loss: 4.4389e-04 - val_categorical_accuracy: 0.9998\n",
            "Epoch 16/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0687 - categorical_accuracy: 0.9829 - val_loss: 0.0281 - val_categorical_accuracy: 0.9935\n",
            "Epoch 17/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0021 - categorical_accuracy: 0.9999 - val_loss: 0.0025 - val_categorical_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 4.9949e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_categorical_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.9345e-04 - categorical_accuracy: 1.0000 - val_loss: 9.9715e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.9517e-04 - categorical_accuracy: 1.0000 - val_loss: 7.0533e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.2935e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8456e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 9.2156e-05 - categorical_accuracy: 1.0000 - val_loss: 3.5436e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0440 - categorical_accuracy: 0.9872 - val_loss: 0.4353 - val_categorical_accuracy: 0.8658\n",
            "Epoch 24/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0099 - categorical_accuracy: 0.9968 - val_loss: 0.0031 - val_categorical_accuracy: 0.9996\n",
            "Epoch 25/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 4.8992e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0018 - val_categorical_accuracy: 0.9998\n",
            "Epoch 26/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.6401e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 0.9998\n",
            "Epoch 27/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.7516e-04 - categorical_accuracy: 1.0000 - val_loss: 7.8710e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.1869e-04 - categorical_accuracy: 1.0000 - val_loss: 5.0819e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 8.0345e-05 - categorical_accuracy: 1.0000 - val_loss: 3.8821e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 5.5086e-05 - categorical_accuracy: 1.0000 - val_loss: 2.3612e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 3.7274e-05 - categorical_accuracy: 1.0000 - val_loss: 1.6282e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "920/920 [==============================] - 4s 4ms/step - loss: 2.5468e-05 - categorical_accuracy: 1.0000 - val_loss: 1.3381e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.6791e-05 - categorical_accuracy: 1.0000 - val_loss: 1.0295e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.1320e-05 - categorical_accuracy: 1.0000 - val_loss: 6.3617e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 7.4606e-06 - categorical_accuracy: 1.0000 - val_loss: 4.1385e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 5.5324e-06 - categorical_accuracy: 1.0000 - val_loss: 3.3805e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 3.1777e-06 - categorical_accuracy: 1.0000 - val_loss: 1.8382e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.0208e-06 - categorical_accuracy: 1.0000 - val_loss: 1.2565e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.3127e-06 - categorical_accuracy: 1.0000 - val_loss: 1.0267e-05 - val_categorical_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 8.5087e-07 - categorical_accuracy: 1.0000 - val_loss: 6.5382e-06 - val_categorical_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0404 - categorical_accuracy: 0.9911 - val_loss: 1.1279 - val_categorical_accuracy: 0.7045\n",
            "Epoch 42/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 0.0287 - categorical_accuracy: 0.9912 - val_loss: 0.0026 - val_categorical_accuracy: 0.9996\n",
            "Epoch 43/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.8507e-04 - categorical_accuracy: 1.0000 - val_loss: 6.8756e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.2544e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8097e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 8.4484e-05 - categorical_accuracy: 1.0000 - val_loss: 3.6551e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 5.9193e-05 - categorical_accuracy: 1.0000 - val_loss: 2.8524e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 4.1211e-05 - categorical_accuracy: 1.0000 - val_loss: 2.2227e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.9238e-05 - categorical_accuracy: 1.0000 - val_loss: 1.6660e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 2.0549e-05 - categorical_accuracy: 1.0000 - val_loss: 1.3057e-04 - val_categorical_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "920/920 [==============================] - 3s 4ms/step - loss: 1.4657e-05 - categorical_accuracy: 1.0000 - val_loss: 6.2098e-04 - val_categorical_accuracy: 0.9998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f34700b2a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0IKq9rnDL0v",
        "colab_type": "text"
      },
      "source": [
        "За 50 эпох удалось достичь 100%-й точности на валидационной выборке!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDd07Vzwf4fM",
        "colab_type": "text"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Примените дополнение данных (_data augmentation_). Как это повлияло на качество классификатора?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nPvDhWtf9EW",
        "colab_type": "text"
      },
      "source": [
        "### Задание 4\n",
        "\n",
        "Поэкспериментируйте с готовыми нейронными сетями (например, _AlexNet_, _VGG16_, _Inception_ и т.п.), применив передаточное обучение. Как это повлияло на качество классификатора? Можно ли было обойтись без него?\n",
        "\n",
        "Какой максимальный результат удалось получить на контрольной выборке?"
      ]
    }
  ]
}